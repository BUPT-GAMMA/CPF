# global configurations
global:
  max_epoch: 500
  patience: 50
  # Disables CUDA training.
  no_cuda: False
  # Validate during training pass.
  fastmode: False
  # divide test data into two parts
  div_test: False
  seed: 0
  # Weight of distillation loss
  loss_alpha: 0.1
  optimizer: 'Adam'
  ground: False

# specific configurations of each model
GCN:
  hidden: 64
  learning_rate: 0.01
  dropout: 0.8
  weight_decay: 0.001
  temp: 1.0
  att: False
  layer_flag: False
GAT:
  num_layers: 2
  hidden: 64
  learning_rate: 0.01
  dropout: 0.6
  att_dropout: 0.3
  alpha: 0.2
  weight_decay: 0.01
  temp: 1.0
  num_heads: 8
  att: True
  layer_flag: False
SGAT:
  num_layers: 2
  hidden: 64
  learning_rate: 0.01
  dropout: 0.6
  att_dropout: 0.3
  alpha: 0.2
  weight_decay: 0.01
  temp: 1.0
  num_heads: 1
  att: True
  layer_flag: False
SpGAT:
  num_layers: 2
  hidden: 64
  learning_rate: 0.01
  dropout: 0.6
  att_dropout: 0.3
  alpha: 0.2
  weight_decay: 0.01
  temp: 1.0
  num_heads: 8
  att: True
  layer_flag: False
SSpGAT:
  num_layers: 2
  hidden: 64
  learning_rate: 0.01
  dropout: 0.6
  att_dropout: 0.3
  weight_decay: 0.01
  temp: 1.0
  num_heads: 1
  att: True
  layer_flag: False
MLP:
  num_layers: 2
  hidden: 64
  learning_rate: 0.005
  dropout: 0.8
  weight_decay: 0.01
  temp: 1.0
  att: False
  layer_flag: False
LogReg:
  learning_rate: 0.005
  dropout: 0.8
  weight_decay: 0.01
  temp: 1.0
  att: False
  layer_flag: False
PLP:
#  num_layers: 10
#  reduced_dim: 64
#  learning_rate: 0.01
#  dropout: 0.8
#  weight_decay: 0.01
#  temp: 1.0
#  coefficient_dropout: 0.1
#  att_dropout: 0.3
#  lr_ratio: 0 # Weight of linear regression
#  beta: 0.
  att: True
  layer_flag: False
SpPLP:
  num_layers: 10
  reduced_dim: 64
  learning_rate: 0.1
  dropout: 0.6
  att_dropout: 0.3
  mlp_dropout: 0.8
  alpha: 0.2 # for leakyrelu
  weight_decay: 0.01
  temp: 1.0
  share_weight: True
  att: True
  layer_flag: False
  lr_ratio: 0 # Weight of linear regression
DeepWalk:
  walk_length: 80
  number_walks: 10
  representation_size: 128
  workers: 8
  window_size: 10
  clf_ratio: 0.5  # The ratio of training data in the classification
  att: False
  layer_flag: False
GraphSAGE:
  agg_type: 'gcn'   # mean/gcn/pool/lstm
  embed_dim: 128
  batch_size: 256
  num_samples: 5
  learning_rate: 0.01
  weight_decay: 0.0005
#  optimizer: 'SGD'
  att: False
  layer_flag: False
APPNP:
  hiddenunits: 64
  drop_prob: 0.5
  alpha: 0.2
  niter: 10
  reg_lambda: 5e-3
  learning_rate: 0.01
  weight_decay: 0.01
  att: False
  layer_flag: False
MoNet:
  learning_rate: 0.003
  weight_decay: 0.001
  att: False
  layer_flag: False
SGC:
  learning_rate: 0.1
  weight_decay: 0.001
  att: False
  layer_flag: False
GCNII:
  learning_rate: 0.01
  wd1: 0.01
  wd2: 0.0005
  layer: 16
  hidden: 64
  dropout: 0.6
  alpha: 0.1
  lamda: 0.5